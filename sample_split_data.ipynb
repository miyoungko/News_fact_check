{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb29649b-c857-4cd8-95df-ea4ca7d6bf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from trankit import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2acc40cc-1451-421d-a40a-2fe98191797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = 'dataset/final_filtered_v0515.json'\n",
    "OUT_FILE = 'dataset/final_filtered_sample_v0515_trankit.json'\n",
    "DIR = 'dataset/sampled/'\n",
    "filter_ =  ['....', 'Advertisement', 'ADVERTISEMENT', 'All rights reserved.']\n",
    "media = [\"Washington Times\", \"CNN (Online News)\", \"New York Times (News)\", 'New York Times (News)',\"Washington Post\", 'BBC News']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9606484b-9edc-4619-bc17-a8f840d9934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def generate_pairs(d, p):    \n",
    "    \n",
    "    d_dict = {}\n",
    "    articles = d['articles']\n",
    "    for a in d['articles']:\n",
    "        art_content2 = []\n",
    "        a_id = 'article_' + a['article_id']\n",
    "        art = p.ssplit(a['content'])\n",
    "        sents = [s['text'] for s in art['sentences']]\n",
    "        sents.append(a['title'])\n",
    "        art_content2.append(sents)\n",
    "        d_dict[a_id] = sents\n",
    "\n",
    "    key = ['premise', 'hypothesis']\n",
    "    id_ = d['story_id']\n",
    "    file_name = DIR + 'topic_' + str(id_) + '.json'\n",
    "    whole_pairs = []\n",
    "    \n",
    "    cont_keys = d_dict.keys()\n",
    "    key_pairs = list(itertools.combinations(cont_keys, 2))\n",
    "    for pair in key_pairs:\n",
    "        idx = pair[0] + '_' + pair[1]\n",
    "        cont0 = d_dict[pair[0]]\n",
    "        cont1 = d_dict[pair[1]]\n",
    "        cont_pairs = list(itertools.product(cont0, cont1))\n",
    "        for j, p in enumerate(cont_pairs):\n",
    "            p_dict ={}\n",
    "            idx_p1 = idx + '_'+ str(2*j)\n",
    "            p_dict[key[0]] = p[0]\n",
    "            p_dict[key[1]] = p[1]\n",
    "            p_dict['idx'] = idx_p1\n",
    "            whole_pairs.append(p_dict)\n",
    "            p_dict = {}\n",
    "            idx_p2 = idx + '_'+ str(2*j + 1)\n",
    "            p_dict[key[0]] = p[1]\n",
    "            p_dict[key[1]] = p[0]\n",
    "            p_dict['idx'] = idx_p2\n",
    "            whole_pairs.append(p_dict)\n",
    "                \n",
    "    with open(file_name, 'w', encoding='utf-8') as f:\n",
    "        json.dump({\"data\":whole_pairs}, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "403e7fba-7fb1-4c68-8425-bc1088853080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(data):\n",
    "\n",
    "    p = Pipeline('english', embedding='xlm-roberta-large', gpu=True, cache_dir='/mnt/.cache')\n",
    "    overall = []\n",
    "\n",
    "    error = ['error code: 404', 'page not found']\n",
    "\n",
    "    for d in tqdm(data):\n",
    "    \n",
    "        articles = []\n",
    "    \n",
    "        for a in d['articles']:\n",
    "            \n",
    "            Error = False\n",
    "            \n",
    "            if a['source'] in media:\n",
    "                continue\n",
    "                \n",
    "            doc = p.ssplit(a['content'])\n",
    "            sents = [s['text'] for s in doc['sentences']]\n",
    "            sents = [s for s in sents if s not in filter_]\n",
    "            print(len(sents))\n",
    "            print(sents)\n",
    "            \n",
    "            if not 4 < len(sents) < 50:\n",
    "                continue\n",
    "            \n",
    "            for e in error:\n",
    "                if e in a['content'].lower():\n",
    "                    Error = True\n",
    "                    print(a['content'])\n",
    "            if Error:\n",
    "                continue\n",
    "            articles.append(a)\n",
    "            \n",
    "            \n",
    "        d['articles'] = articles\n",
    "        if len(d['articles']) > 1:\n",
    "            overall.append(d)\n",
    "            generate_pairs(d, p)\n",
    "\n",
    "    print(len(overall))\n",
    "    return overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8479a564-a17e-4e42-8513-dde98718283f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(FILE, 'r')) \n",
    "new_data = filter_data(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01170351-afb8-4fb5-b829-48d07acf5265",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUT_FILE, 'w') as f:\n",
    "    json.dump(new_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83436bbc-3e0d-4703-a990-247dc3da4a22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
